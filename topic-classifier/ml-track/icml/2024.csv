uuid,title,abstract,response
icml_2024_0,"Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024.",,No
icml_2024_1,Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo.,,No
icml_2024_2,Position: Open-Endedness is Essential for Artificial Superhuman Intelligence.,,No
icml_2024_3,Stop Regressing: Training Value Functions via Classification for Scalable Deep RL.,,No
icml_2024_4,Improving Transformers with Dynamically Composable Multi-Head Attention.,,No
icml_2024_5,Learning Useful Representations of Recurrent Neural Network Weight Matrices.,,No
icml_2024_6,Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model.,,No
icml_2024_7,SceneCraft: An LLM Agent for Synthesizing 3D Scenes as Blender Code.,,No
icml_2024_8,Doubly Robust Causal Effect Estimation under Networked Interference via Targeted Learning.,,No
icml_2024_9,Emergent Equivariance in Deep Ensembles.,,No
icml_2024_10,Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks.,,No
icml_2024_11,Position: Automatic Environment Shaping is the Next Frontier in RL.,,No
icml_2024_12,Online Matching with Stochastic Rewards: Provable Better Bound via Adversarial Reinforcement Learning.,,No
icml_2024_13,"Position: Beyond Personhood: Agency, Accountability, and the Limits of Anthropomorphic Ethical Analysis.",,No
icml_2024_14,Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape.,,No
icml_2024_15,Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews.,,No
icml_2024_16,Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics.,,No
icml_2024_17,Unified Training of Universal Time Series Forecasting Transformers.,,No
icml_2024_18,Position: Opportunities Exist for Machine Learning in Magnetic Fusion Energy.,,No
icml_2024_19,Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models.,,No
